When building a deep-learning model, you have to make many seemingly arbitrary decisions: How many layers should you stack? How many units or filters should go in each layer? Should you use relu as activation, or a different function? Should you use BatchNormalization after a given layer? How much dropout should you use? And so on. These architecture-level parameters are called hyperparameters to distinguish them from the parameters of a model, which are trained via backpropagation.
In practice, experienced machine-learning engineers and researchers build intu- ition over time as to what works and what doesn’t when it comes to these choices— they develop hyperparameter-tuning skills. But there are no formal rules. If you want to get to the very limit of what can be achieved on a given task, you can’t be content with arbitrary choices made by a fallible human. Your initial decisions are almost always suboptimal, even if you have good intuition. You can refine your choices by tweaking them by hand and retraining the model repeatedly—that’s what machine- learning engineers and researchers spend most of their time doing. But it shouldn’t be your job as a human to fiddle with hyperparameters all day—that is better left to a machine.
Thus you need to explore the space of possible decisions automatically, systemati- cally, in a principled way. You need to search the architecture space and find the best- performing ones empirically. That’s what the field of automatic hyperparameter opti- mization is about: it’s an entire field of research, and an important one.
The process of optimizing hyperparameters typically looks like this:

1 Choose a set of hyperparameters (automatically).
2 Build the corresponding model.
3 Fit it to your training data, and measure the final performance on the valida-
tion data.
4 Choose the next set of hyperparameters to try (automatically).
5 Repeat.
6 Eventually, measure performance on your test data.
The key to this process is the algorithm that uses this history of validation perfor- mance, given various sets of hyperparameters, to choose the next set of hyperparame- ters to evaluate. Many different techniques are possible: Bayesian optimization, genetic algorithms, simple random search, and so on.
Training the weights of a model is relatively easy: you compute a loss function on a mini-batch of data and then use the Backpropagation algorithm to move the weights

CHAPTER 7 Advanced deep-learning best practices
in the right direction. Updating hyperparameters, on the other hand, is extremely
challenging. Consider the following:
 Computing the feedback signal (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive: it requires creating and training a new model from scratch on your dataset.
 The hyperparameter space is typically made of discrete decisions and thus isn’t continuous or differentiable. Hence, you typically can’t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent.
Because these challenges are difficult and the field is still young, we currently only have access to very limited tools to optimize models. Often, it turns out that random search (choosing hyperparameters to evaluate at random, repeatedly) is the best solu- tion, despite being the most naive one. But one tool I have found reliably better than random search is Hyperopt (https://github.com/hyperopt/hyperopt), a Python library for hyperparameter optimization that internally uses trees of Parzen estimators to predict sets of hyperparameters that are likely to work well. Another library called Hyperas (https://github.com/maxpumperla/hyperas) integrates Hyperopt for use with Keras models. Do check it out.
NOTE One important issue to keep in mind when doing automatic hyperpa- rameter optimization at scale is validation-set overfitting. Because you’re updating hyperparameters based on a signal that is computed using your vali- dation data, you’re effectively training them on the validation data, and thus they will quickly overfit to the validation data. Always keep this in mind.
Overall, hyperparameter optimization is a powerful technique that is an absolute requirement to get to state-of-the-art models on any task or to win machine-learning competitions. Think about it: once upon a time, people handcrafted the features that went into shallow machine-learning models. That was very much suboptimal. Now, deep learning automates the task of hierarchical feature engineering—features are learned using a feedback signal, not hand-tuned, and that’s the way it should be. In the same way, you shouldn’t handcraft your model architectures; you should optimize them in a principled way. At the time of writing, the field of automatic hyperparame- ter optimization is very young and immature, as deep learning was some years ago, but I expect it to boom in the next few years.
